#### A. Deployments
(+)  K8S requires  a supervisor program that will continually checks that the container is running, and if it ever stops, start it again immediately, for this:
- `Traditional servers` have `systemd`, `runit`, or `supervisord`
- `Docker` has `--restart`
- `K8S` has `deployment`

![[Pasted image 20240416132646.png]]

(+) `Deployment` is blueprint for `my-app` pods
(+) We create `Deployment` and manage other components through it, it will act as an abstraction of Pods.

![[Pasted image 20240612133014.png | center]]

==> `DB` are often hosted outside of K8S.

#### B. ReplicaSets
(+) `Deployment` dont manage `Pods` directly. It does that through the mean of `ReplicaSets` Object.
(+) A `ReplicaSet` is responsible for a group of identical Pods, or `replicas`. 
==> If there are too few `Pods`, compared to the specification or vice versa, the `ReplicaSet` controller will start or stop some `Pods` to rectify the situation.

![[Pasted image 20240416130257.png]]

###### Maintaining a desired state
(+) K8S controllers continually check the desired state specified by each resource against the actual state of the cluster, and make any adjustment to keep them in sync.
==> This is called `reconciliation loop`, because it loop forever, trying to reconcile the actual state with the desired state.

![[Pasted image 20240416130316.png]]

==> We can still shut the deployment down and clear the environment using `kubectl delete all --selector app=demo`

![[Pasted image 20240416130921.png | center]]


#### C. What is in side a Node?
###### C1. Pods
(+) A pod is the K8S object that represents a group of one or more containers.
(+) Sometimes a set of container needs to be scheduled together, running on the same node, and communicating locally, perhaps sharing storage, therefore, using a `deployment` to manage them would not be ideal.
==> A blog application might have one container that syncs content with a Git repos, and a Nginx web sever container that serves the blog content to users. `Since they share data, both containers need to be scheduled together in a Pod`.

![[Pasted image 20240416132703.png]]


###### C2. Service and Ingress
(+) Service is permanent IP address, and the lifecycle of Pod and Service are note connected, it can also act as a load balancer.
(+) `Service` can be accessed through `IP Address` and `Port`, The `IP` is usually owned by the node, and the `Port` is owned by the service. However, we would want to use the `URL` not the IP and port.
(+) `Ingress` is used as a table of translation between `URL` and `IP + Port` so that we can use our URL without the IP.

![[Pasted image 20240612131700.png | center]]

###### C3. Config_map and Secret
(+) `ConfigMap` is an external configuration of your application for example
+ DB_URL = mongo-db

(+) `Secret` is used to store secret data in base64 encoded. But this is not enabled by default.
+ DB_USER = mongo-user
+ DB_PWD = mongo-pwd

![[Pasted image 20240612132336.png | center]]

###### C4. Volume
(+) `Database` and `Volume` are 2 different things, we should not store our data on database as if it is reset, everything will be lost.
(+) `Volumes` is a physical storage on `local` or `remote` and it will be attached to the database. Then even if the database is reset, our data will not be lost.

![[Pasted image 20240612132604.png | center]]
















#### D. The K8S Scheduler
(+) The K8S scheduler Ã­ the component responsible for this part of the process, when a `Deployment` decides that a new replica is needed, it creates a Pod resource in the K8S database, Simultaneously, this Pod is added to a queue, which is like the scheduler's inbox

(+) The scheduler's job is to watch its queue of unscheduled Pods, grab the next Pod from it and find a node to run it on.

(+) Pod has been scheduled on a node, the `kubectl` running on that node picks it up and takes care of actually starting its container

#### E. Resource Manifests in YAML Format
(+) K8S is inherently a `declarative system`, continuously reconciling acutal state with desired state.
(+) All K8S resources are represented by records in its internal database. The reconciliation loop watches the database for any changes to those records. 
==> All `kubectl run` command does is add a new record in the database corresponding to a `deployment`, and K8S does the rest.

###### Deployment Manifest
(+) The usual format for K8S manifest files is YAML

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: demo
  labels:
    app: demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: demo
  template:
    metadata:
      labels:
        app: demo
    spec:
      containers:
        - name: demo
          image: cloudnatived/demo:hello
          ports:
          - containerPort: 8888
```

(+) We can then use `kubectl apply` to submit the manifest to the cluster
==> `kubectl apply -f k8s/deployment.yaml`
==>  `kubectl get pods --selector app=demo`

(+) This however, only create the deployment and its pods, we needs to connect these resources with a service to allow its usage.

#### F. Service Resources
(+) A service resource gives us a single, unchanging IP address or DNS name that will be automatically routed to any matching Pod.
(+) Service can be thought as a web proxy or a load baalncer, forwarding requests to a set of backend Pods.

```yaml
apiVersion: v1
kind: Service
metadata:
  name: demo
  labels:
    app: demo
spec:
  ports:
  - port: 8888
    protocol: TCP
    targetPort: 8888
  selector:
    app: demo
  type: ClusterIP
```

(+) The `selector` is the part that tells the Service how to route requests to a particular Pods.

==> `Deployment manages a set of Pods for your application, and a Service gives you a single entry point for requests to these Pods`

(+) `kubectl apply -f k8s/service.yaml`
(+) `kubectl port-forward service/demo 9999:8888`