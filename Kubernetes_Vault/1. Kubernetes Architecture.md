#### A. What is Kubernetes
(+) Kubernetes is a tool for automated management of containerized applications also known as a container orchestration tool
==> It is used to automate application infrastructure and make it easy to manage

#### B. Cluster Architecture
###### 1. The Control Plane - Master Process
(+) The Cluster's brain is called the `control plane`, it runs tasks required for K8S to do its job: `scheduling containers`,  `managing services`, `serving API request`. It is usually run on a master node.

(+) The control plane is made up of several components:
- `kube-apiserver:` The front-end server for the control plane, handling API request. This could be an GUI or an CLI.
![[Pasted image 20240612134813.png | center]]

- `kube-scheduler:` this decides where to run newly created pods, it then will forward request to the designated pods where `Kubelet` on the node will run the new pods.
![[Pasted image 20240612134858.png]]

- `kube-controller-manager:` responsible for running resource controllers, such as Deployment. It will detects cluster state changes and make appropriate changes. 
![[Pasted image 20240612135126.png | center]]

- `etcd:` The database where K8S stores all of its information about nodes and resources, it can be thought of as a `cluster brain`. And every changes to the cluster will get stored in this data. Application is not stored in `etcd`
![[Pasted image 20240612135327.png |center]]

- `cloud-controller-manager:` Interact with cloud provider

###### 2. Node Components - Worker nodes
(+) Cluster members that run user workloads are called worker nodes, each worker node runs these components:
- `kubelet:` This is responsible for driving the container runtime to start workloads and monitor their status
- `kube-proxy:` This node is responsible for the networking that routes request between Pods on different nodes.
- `Container runtime`: This  starts and stops containers and handles their communication.

(+) On every worker nodes, there must be a `container runtime`, therefore, `container runtime` is configured on every worker nodes. `Kubelet` interacts with both - the container and node, `Kubelet` states the pod with container inside.
![[Pasted image 20240612133740.png | center]]

(+) `K8S Cluster` is usually made up of multiple worker nodes, worker nodes in a cluster need to be connected, there needs to be a process of forwarding request to pods, `kube-proxy` forwards the requests.
![[Pasted image 20240612134155.png | center]]

###### 3. Example Cluster Set-up
(+) An example setup would be 2 master nodes and 3 worker nodes. We can add more resources should request increases.
![[Pasted image 20240612135546.png]]

###### 4. How to test local cluster setup using Minikube
(+) `Minikube` is when master and node processes run on `One machine`
(+) Creates Virtual Box on your laptop, node is then runs in that Virtual Box.
![[Pasted image 20240612135950.png | center]]


```#!bin/bash
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube_latest_amd64.deb
sudo dpkg -i minikube_latest_amd64.deb
sudo snap install kubectl --classic

#minikube start --driver=docker
```




#### C. Cluster High Availability
(+) A correctly configured K8S control plane has multiple master nodes, making it highly available.
(+) A HA control plane will also handle the situation where the master nodes have network partition - meaning network failure.
(+) The `etcd` database is replicated across multiple nodes, and can survive the failure of an individual node.

###### 1. Control plane failure
(+) A damaged control plane does not necessarily mean that your application will go down
(+) If u were to stop all the master nodes in your cluster, the worker nodes would keep on running, however, management actions such as deploying new container or changing resources would not be possible.

###### 2. Worker node failure
(+) The failure of any worker node does not really matter, as K8S will detect the failure and reschedule the node's Pods somewhere else, as long as the control plane is still working.
(+) A rare, but entirely possible, kind of failure is losing a whole cloud `availability zone`, therefore, it is a good idea to distribute the worker nodes across 2 to 3 zones.